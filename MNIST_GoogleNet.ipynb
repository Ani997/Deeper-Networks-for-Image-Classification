{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_GoogleNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0fnXg0OKIrd"
      },
      "source": [
        "# Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import datetime as datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcflLDJNKLKh"
      },
      "source": [
        "# Data pre-processing\n",
        "transform_train = transforms.Compose([transforms.Resize((70)), # Resizing the dataset samples to 70x70\n",
        "                                transforms.RandomHorizontalFlip(), # Randomly flips the data samples horizontally\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5),(0.5))]) # Normalizing the dataset\n",
        "\n",
        "transform_test = transforms.Compose([transforms.Resize((70)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5),(0.5))])\n",
        "\n",
        "# Downloading and loading the dataset into the workspace\n",
        "train = torchvision.datasets.MNIST(root='./data',train=True, transform=transform_train, download=True)\n",
        "test = torchvision.datasets.MNIST(root='./data',train=False, transform=transform_test, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=32, shuffle=False)\n",
        "cuda = torch.device('cuda') # to train the model on a GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtvyWWbGKQOi"
      },
      "source": [
        "# Structure of the GoogleNet architecture\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self,in_channel,c1,c2,c3,c4):\n",
        "        super(Inception,self).__init__()\n",
        "        self.norm1_1=nn.BatchNorm2d(in_channel,eps=1e-3)\n",
        "        self.p1_1=nn.Conv2d(in_channels=in_channel,out_channels=c1,kernel_size=1)\n",
        "        self.norm2_1 = nn.BatchNorm2d(in_channel, eps=1e-3)\n",
        "        self.p2_1=nn.Conv2d(in_channels=in_channel,out_channels=c2[0],kernel_size=1)\n",
        "        self.norm2_2 = nn.BatchNorm2d(c2[0], eps=1e-3)\n",
        "        self.p2_2=nn.Conv2d(in_channels=c2[0],out_channels=c2[1],kernel_size=3,padding=1)\n",
        "        self.norm3_1 = nn.BatchNorm2d(in_channel, eps=1e-3)\n",
        "        self.p3_1=nn.Conv2d(in_channels=in_channel,out_channels=c3[0],kernel_size=1)\n",
        "        self.norm3_2 = nn.BatchNorm2d(c3[0], eps=1e-3)\n",
        "        self.p3_2=nn.Conv2d(in_channels=c3[0],out_channels=c3[1],kernel_size=5,padding=2)\n",
        "        self.p4_1 = nn.MaxPool2d(kernel_size=3,stride=1,padding=1)\n",
        "        self.norm4_2 = nn.BatchNorm2d(in_channel, eps=1e-3)\n",
        "        self.p4_2 = nn.Conv2d(in_channels=in_channel, out_channels=c4, kernel_size=1)\n",
        " \n",
        "    def forward(self, x):\n",
        "        p1=self.p1_1(F.relu(self.norm1_1(x)))\n",
        "        p2=self.p2_2(F.relu(self.norm2_2(self.p2_1(F.relu(self.norm2_1(x))))))\n",
        "        p3=self.p3_2(F.relu(self.norm3_2(self.p3_1(F.relu(self.norm3_1(x))))))\n",
        "        p4=self.p4_2(F.relu(self.norm4_2(self.p4_1(x))))\n",
        "        return torch.cat((p1,p2,p3,p4),dim=1)\n",
        "\n",
        "class GoogleNet(nn.Module):\n",
        "    def __init__(self,in_channel,num_classes):\n",
        "        super(GoogleNet,self).__init__()\n",
        "        layers=[]\n",
        "        layers+=[nn.Conv2d(in_channels=in_channel,out_channels=64,kernel_size=7,stride=2,padding=3),\n",
        "                 nn.ReLU(),\n",
        "                 nn.MaxPool2d(kernel_size=3,stride=2,padding=1)]\n",
        "        layers+=[nn.Conv2d(in_channels=64,out_channels=64,kernel_size=1),\n",
        "                 nn.Conv2d(in_channels=64,out_channels=192,kernel_size=3,padding=1),\n",
        "                 nn.MaxPool2d(kernel_size=3,stride=2,padding=1)]\n",
        "        layers+=[Inception(192,64,(96,128),(16,32),32),\n",
        "                 Inception(256,128,(128,192),(32,96),64),\n",
        "                 nn.MaxPool2d(kernel_size=3,stride=2,padding=1)]\n",
        "        layers+=[Inception(480, 192, (96, 208), (16, 48), 64),\n",
        "                 Inception(512, 160, (112, 224), (24, 64), 64),\n",
        "                 Inception(512, 128, (128, 256), (24, 64), 64),\n",
        "                 Inception(512, 112, (144, 288), (32, 64), 64),\n",
        "                 Inception(528, 256, (160, 320), (32, 128), 128),\n",
        "               nn.MaxPool2d(kernel_size=3, stride=2, padding=1)]\n",
        "        layers += [Inception(832, 256, (160, 320), (32, 128), 128),\n",
        "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
        "                   nn.AvgPool2d(kernel_size=2)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.dense=nn.Linear(1024,num_classes)\n",
        " \n",
        " \n",
        "    def forward(self,x):\n",
        "        x=self.net(x)\n",
        "        x=x.view(-1,1024*1*1)\n",
        "        x=self.dense(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Using CUDA enabled GPU for training the model\n",
        "model = GoogleNet(1, 10).to(device) # Setting the number of input classes = 1 and output classes = 10\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdlqqfD2KXSo"
      },
      "source": [
        "loss = nn.CrossEntropyLoss() # Using Cross Entropy as Loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # Setting the optimizer for training the model\n",
        "cost = 0\n",
        "epochs = 5 # Setting the number of epochs for which the model is to be trained\n",
        "\n",
        "iterations = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for X, Y in train_loader:\n",
        "        X = X.to(cuda)\n",
        "        Y = Y.to(cuda)\n",
        "        optimizer.zero_grad()\n",
        "        hypo = model(X)\n",
        "        cost = loss(hypo, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        prediction = hypo.data.max(1)[1]\n",
        "        correct += prediction.eq(Y.data).sum()\n",
        "\n",
        "    model.eval()\n",
        "    correct2 = 0\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(cuda)\n",
        "        target = target.to(cuda)\n",
        "        output = model(data)\n",
        "        cost2 = loss(output, target)\n",
        "        prediction = output.data.max(1)[1]\n",
        "        correct2 += prediction.eq(target.data).sum()\n",
        "\n",
        "    iterations.append(epoch)\n",
        "    train_losses.append(cost.tolist())\n",
        "    test_losses.append(cost2.tolist())\n",
        "    train_acc.append((100*correct/len(train_loader.dataset)).tolist())\n",
        "    test_acc.append((100*correct2/len(test_loader.dataset)).tolist())\n",
        "    print(\"Epoch : {:>4} / cost : {:>.9}\".format(epoch + 1, cost))\n",
        "    print('Train set Accuracy: {:.2f}%'.format(100. * correct / len(train_loader.dataset)))\n",
        "    print('Test set Accuracy: {:.2f}%'.format(100. * correct2 / len(test_loader.dataset)))\n",
        "    timestamp = datetime.datetime.now()\n",
        "    print(\"Date/Time stamp\", timestamp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC3QmwXBKesp"
      },
      "source": [
        "# Train Accuracy vs Validation accuracy plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_acc, color='green', label='train accuracy')\n",
        "plt.plot(test_acc, color='blue', label='validataion accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('accuracy.png')\n",
        "plt.show()\n",
        "\n",
        "# Train Loss vs Validation Loss plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_losses, color='orange', label='train loss')\n",
        "plt.plot(test_losses, color='red', label='validataion loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZhM3VyWKjgJ"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Utilizng CUDA enabled GPU\n",
        "print(device)\n",
        "classes = ['0','1','2','3','4','5','6','7','8','9'] # Classes of the MNIST dataset\n",
        "\n",
        "# Building the heatmap of the correctly classified and misclassified data samples\n",
        "heatmap = pd.DataFrame(data=0,index=classes,columns=classes)\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(16):\n",
        "            true_label = labels[i].item()\n",
        "            predicted_label = predicted[i].item()\n",
        "            heatmap.iloc[true_label,predicted_label] += 1\n",
        "_, ax = plt.subplots(figsize=(10, 8))\n",
        "ax = sns.heatmap(heatmap, annot=True, fmt='d',cmap='YlGnBu')\n",
        "figure = ax.get_figure()    \n",
        "figure.savefig('heatmap.png', dpi=400)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mvnkMAjKo0J"
      },
      "source": [
        "# Saving the model\n",
        "torch.save(model,'GoogleNet_CIFAR10.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}